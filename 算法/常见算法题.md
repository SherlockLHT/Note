### 给定A、B两个文件，每个存放50亿个url，每个url占64个字节，内存限制4G，找出两个文件中相同的url

50亿*64字节，大约需要298G内存，两个文件又要翻倍， 则不能常规解决，使用时间换空间的方法，步骤如下：

1. 分别读取两个文件，计算每个url的 **hash** 值，并 **%1000** 取余，将此url保存在余数的文件中，注意两个文件区分开，最后会得到 **1000*2** 个文件，假设分别是A0、A1...和B0、B1...；
2. 对比两批相对应的文件，即A0和B0、A1和B1，以此类推，找到相同的url；
3. 在遍历两批文件的时候，可以使用map，将A0的url保存在map中，B0的url在map中查找即可；
4. 这些处理过程可能会很耗时，可以开多个线程同时操作；

**说明：**

这是一种 **分治** 的思想，因为原始文件太大，根据唯一的特性，将其分类到很多个小文件中。

**分治** 即分而治之，在我们的日常生活中也会常用到，最常见的就是分扑克牌。经常会有这种时候，两幅扑克牌已经混合在一起了，现在需要将其分开，而一双手无法展开所有的牌进行对比。我们通常的做法是，合起所有的牌，一张一张翻开，将此张牌根据牌面数字放在一堆中，最后我们会得到14组的牌堆。然后，我们再挨个从每组牌堆中取出各种花色，最后放在一起，就分开了我们需要的两副牌。

上述方法有几点需要注明：

1. 关于取余，若不取余，则会生成50亿个文件，取余即是特征提取，提取这些url中的某些共有特性，进行分类，而和哪个数取余则直接决定最后生成文件的数量；
2. 关于文件对比，相同的url一定会在对应的文件中，而对应的文件中的url不一定相同。如果url相同，则得到的hash值取余之后一定相同，即他们一定会在An或者Bn中，所以，对比文件的时候，只要对比对应的文件即可；
3. map内的数据不能重复，利用这个特性，则可以轻松得到相同的url；

